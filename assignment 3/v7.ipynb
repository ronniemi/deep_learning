{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Input, Embedding, Lambda, Dense, concatenate, Flatten\nfrom keras.layers.normalization import BatchNormalization\nimport keras.backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport datetime\nfrom time import time\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Using TensorFlow backend.\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "['home-depot-product-search-relevance', 'gnewsvector']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "510b1e325351c39896012f8eaae8e870d3cc991a"
      },
      "cell_type": "markdown",
      "source": "# Read the data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/home-depot-product-search-relevance/train.csv', encoding=\"ISO-8859-1\")\ntest = pd.read_csv('../input/home-depot-product-search-relevance/test.csv', encoding=\"ISO-8859-1\")\n# prod = pd.read_csv('../input/home-depot-product-search-relevance/product_descriptions.csv')\n\nprint('train size', train.shape)\nprint('test size', test.shape)\n# print('prod size', prod.shape)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "train size (74067, 5)\ntest size (166693, 4)\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "920a26c0a9ce905505be5ae5a86de6233c8a863e"
      },
      "cell_type": "code",
      "source": "train_size = train.shape[0]\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\n# df_all = pd.merge(df_all, prod, on='product_uid', how='left')\n# df_all['product_title'] = (df_all['product_title'] + df_all['product_description'])\n# df_all.drop(['product_description', 'product_uid', 'id'], axis=1, inplace=True)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\nof pandas will change to not sort by default.\n\nTo accept the future behavior, pass 'sort=False'.\n\nTo retain the current behavior and silence the warning, pass 'sort=True'.\n\n  \n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b295c37257082cf282bbe3a85bff98a996aa42d"
      },
      "cell_type": "code",
      "source": "df_all.head()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "   id         ...                 search_term\n0   2         ...               angle bracket\n1   3         ...                   l bracket\n2   9         ...                   deck over\n3  16         ...            rain shower head\n4  17         ...          shower only faucet\n\n[5 rows x 5 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>product_title</th>\n      <th>product_uid</th>\n      <th>relevance</th>\n      <th>search_term</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>100001</td>\n      <td>3.00</td>\n      <td>angle bracket</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n      <td>100001</td>\n      <td>2.50</td>\n      <td>l bracket</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>9</td>\n      <td>BEHR Premium Textured DeckOver 1-gal. #SC-141 ...</td>\n      <td>100002</td>\n      <td>3.00</td>\n      <td>deck over</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>16</td>\n      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n      <td>100005</td>\n      <td>2.33</td>\n      <td>rain shower head</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>17</td>\n      <td>Delta Vero 1-Handle Shower Only Faucet Trim Ki...</td>\n      <td>100005</td>\n      <td>2.67</td>\n      <td>shower only faucet</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "648d014248f575ff76a50465f31c92b6edbeb56a"
      },
      "cell_type": "markdown",
      "source": "# Check max product description and search term length to know how much padding needed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b399f090841e148561c7f14256e52fa4f946b1a1"
      },
      "cell_type": "code",
      "source": "def get_max_length(data):\n    max_len = 0\n    for i in range(0, len(data)):\n        n_words = len(data.iloc[i].split())\n        if n_words > max_len:\n            max_len = n_words\n    return max_len\n\nmax_length_prod = get_max_length(df_all['product_title'])\nmax_length_search = get_max_length(df_all['search_term'])\n\nprint('max_length_prod', max_length_prod)\nprint('max_length_search', max_length_search)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "max_length_prod 38\nmax_length_search 14\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "d6a1dad7c20f6e167fa6d00f9d4290e1e2ce383b"
      },
      "cell_type": "markdown",
      "source": "# Word embedding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56096b03c2aa3d396b845802e90415da5cb5e589"
      },
      "cell_type": "code",
      "source": "embed_size = 300\nmax_features = 50000 \n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features, filters='!\"$&()*+,-.:;<=>?[\\]^_`{|}~')\ntokenizer.fit_on_texts(list(df_all['product_title'].append(df_all['search_term']).values))\n\nX_train_prod = tokenizer.texts_to_sequences(train['product_title'].values)\nX_train_search = tokenizer.texts_to_sequences(train['search_term'].values)\nX_test_prod = tokenizer.texts_to_sequences(test['product_title'].values)\nX_test_search = tokenizer.texts_to_sequences(test['search_term'].values)",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b691cb1a543d0ad4cb2ef8e03c853f997071456"
      },
      "cell_type": "markdown",
      "source": "# Padding by the max_length_prod and max_length_search"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da8304d8b1ef5bbcb6dfcdba45d3a5f3286501d4"
      },
      "cell_type": "code",
      "source": "X_train_prod = pad_sequences(X_train_prod, maxlen=max_length_prod)\nX_train_search = pad_sequences(X_train_search, maxlen=max_length_search)\nX_test_prod = pad_sequences(X_test_prod, maxlen=max_length_prod)\nX_test_search = pad_sequences(X_test_search, maxlen=max_length_search)",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b92ef9663b8889e61b6f36f45c1991cb99bbe13d"
      },
      "cell_type": "markdown",
      "source": "# Assign embedding to words by GoogleNews-vectors-negative300 embedding "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f4739806c196429a563d11056338e225e6e42aa"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/gnewsvector/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nembedding_matrix[0] = 0\nfor word, i in word_index.items():\n    if i >= max_features: \n        continue\n    if word in embeddings_index:\n        embedding_matrix[i] = embeddings_index.get_vector(word)\n\ndel EMBEDDING_FILE\ngc.collect()",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "0"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "e88a1554afea52cd69c63ff69c975d42037271ff"
      },
      "cell_type": "markdown",
      "source": "# Build the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2f34839c9c3706a8677726481094437e2f19c8"
      },
      "cell_type": "code",
      "source": "def get_model():\n\n    # input layer\n    left_input = Input(shape=(max_length_prod, ))\n    right_input = Input(shape=(max_length_search, ))\n\n    # LSTM layer\n    shared_lstm = LSTM(n_lstm_hidden)\n    left_output = shared_lstm(left_input)\n    right_output = shared_lstm(right_input)\n\n    # concat two outputs\n    concat = concatenate([left_output, right_output])\n\n    # add Dense layer to calculate the similarty between product title and search term\n    output = Dense(1)(concat)\n\n    # Pack it all up into a model\n    siamese_model = Model([left_input, right_input], output)\n\n    # colmpiling\n    siamese_model.compile(loss='mse', optimizer='adam')\n    return siamese_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d61c9b10b54462b459d573b29f089aecb2a84530"
      },
      "cell_type": "code",
      "source": "def split_to_train_val(data_x, data_y, train_index, val_index):\n    train_left = data_x['left'][train_index]\n    train_right = data_x['right'][train_index]\n    y_train = data_y[train_index]  \n    val_left = data_x['left'][val_index]\n    val_right = data_x['right'][val_index]\n    y_val = data_y[val_index]\n    \n    return train_left, train_right, val_left, val_right, y_train, y_val",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8003751bc2abcf73d9ea6461755be733f1f5fd23"
      },
      "cell_type": "code",
      "source": "# Plot loss\ndef plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ec48d09772a06ec47042d24ce9cea5dfa63ecee"
      },
      "cell_type": "markdown",
      "source": "# Use the model we got as feature extractor \n1. for XGBoost model\n2. for lightgbm model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a464f5fb8aa73d1e27af90e336448cb927eae0d2"
      },
      "cell_type": "code",
      "source": "# Train the model on all the data\nsiamese_model = get_model()\nhistory = siamese_model.fit([X_data['left'], X_data['right']], y_data.values, batch_size=batch_size, epochs=n_epoch)\n\n# get the output of the concat layer and use it as features to the ml models\nconcat_layer = siamese_model.layers[3].output\nfeature_model = Model(siamese_model.input, concat_layer)\nfeature_model.compile(loss='mse', optimizer='adam')\nprint(feature_model.summary())\n\n# we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\nfeaturs = feature_model.predict([X_data['left'], X_data['right']])\n\n# we preform the prediction also on the test set to evaluate the mse on test set\nfeatures_test = feature_model.predict([X_test['left'], X_test['right']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d1512a3193ea30a0c355380709e1df3c13fa8a"
      },
      "cell_type": "code",
      "source": "# # get the output of the concat layer and use it as features to the ml models\n# concat_layer = siamese_model.layers[3].output\n# feature_model = Model(siamese_model.input, concat_layer)\n# feature_model.compile(loss='mse', optimizer='adam')\n# print(feature_model.summary())\n\n# # we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\n# featurs = feature_model.predict([X_train_left_kfold, X_train_right_kfold])\n\n# # we preform the prediction also on the test set to evaluate the mse on test set\n# features_test = feature_model.predict([X_val_left_kfold, X_val_right_kfold])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b0545941c07162e1d4ce35324ec2a1c7c8c1f43"
      },
      "cell_type": "code",
      "source": "# xgb\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.01, gamma=0, subsample=0.8, colsample_bytree=1, max_depth=7)\nxgb_model.fit(featurs, y_data)\nxgb_pred = xgb_model.predict(features_test)\n\n# lgb\nlgb_model = lgb.sklearn.LGBMRegressor(is_unbalance=True, learning_rate =0.01, subsample=0.8, colsample_bytree=0.6, max_depth=7)\nlgb_model.fit(featurs, y_data)\nlgb_pred = lgb_model.predict(features_test)\n\n\n# xgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, xgb_pred))\n# lgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, lgb_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a863db3a06ab2c990ecbc8e4c68816d30a20a915"
      },
      "cell_type": "code",
      "source": "def round_pred(pred):\n    for i in range(0,len(pred)):\n        if pred[i] < 1:\n            pred[i] = 1\n        if pred[i] > 3:\n            pred[i] = 3\n    return pred\n\nxgb_pred = round_pred(xgb_pred.astype(float))\nlgb_pred = round_pred(lgb_predastype(float))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3198b356da0e4e4576616abcc58b245e4d4f04df"
      },
      "cell_type": "code",
      "source": "# print(xgb_rmse, lgb_rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4dffbbe091404ca00cda0de1986165d7742dbf69"
      },
      "cell_type": "markdown",
      "source": "# Write predictions to sample submission file"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0359113ee85810ef14a496c562709e5d35a1d290"
      },
      "cell_type": "code",
      "source": "sample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred_test\nsample_sub.to_csv('sample_submmision_char_siamese.csv', index=False)\n\nsample_sub['relevance'] = xgb_pred\nsample_sub.to_csv('sample_submmision_char_xgb.csv', index=False)\n\nsample_sub['relevance'] = lgb_pred\nsample_sub.to_csv('sample_submmision_char_lgb.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}