{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Input, Embedding, Lambda, Dense, concatenate, Flatten, Dropout\nfrom keras.layers.normalization import BatchNormalization\nimport keras.backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport datetime\nfrom time import time\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "510b1e325351c39896012f8eaae8e870d3cc991a"
      },
      "cell_type": "markdown",
      "source": "# Read the data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/train.csv', encoding=\"ISO-8859-1\")\ntest = pd.read_csv('../input/test.csv', encoding=\"ISO-8859-1\")\n# prod = pd.read_csv('../input/home-depot-product-search-relevance/product_descriptions.csv')\n\nprint('train size', train.shape)\nprint('test size', test.shape)\n# print('prod size', prod.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "920a26c0a9ce905505be5ae5a86de6233c8a863e"
      },
      "cell_type": "code",
      "source": "train_size = train.shape[0]\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\n# df_all = pd.merge(df_all, prod, on='product_uid', how='left')\n# df_all['product_title'] = (df_all['product_title'] + df_all['product_description'])\n# df_all.drop(['product_description', 'product_uid', 'id'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b295c37257082cf282bbe3a85bff98a996aa42d"
      },
      "cell_type": "code",
      "source": "df_all.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7b49aaa94c7188202018356fafe950c882945da3"
      },
      "cell_type": "markdown",
      "source": "# Split product_full_info and search_term to chars"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ddb5429f76f21507438b4f34f3ff7166cae5621"
      },
      "cell_type": "code",
      "source": "char_to_remove = [' ', '{', '}', '\"', '(', ')', '.', ',', '&','[',']','`','_','\\'', '~', '\\\\', '-' \n                  '\\x80', '\\x81', '\\x84', '\\x89', '\\x8b', '\\x90', '\\x93', '\\x95', '\\x99', '\\x9a', \n                  '\\x9d', '\\xa0', '¡', '¢', 'ª', 'À', 'Â', 'È', 'Ê', 'Ë', 'Ï', 'Ò', 'Û', \n                  'Ü', 'â', 'ã', 'å', 'è', '÷']\n\ndef lower_char(char):\n    if char >= 'A' and char <= 'Z':\n        return char.lower()\n    else:\n        return char\n\ndf_all['product_title'] = df_all['product_title'].apply(lambda sent: [lower_char(character) for character in sent if character not in char_to_remove])\ndf_all['search_term'] = df_all['search_term'].apply(lambda sent: [lower_char(character) for character in sent if character not in char_to_remove])\n\n# df_all['product_title'] = df_all['product_title'].apply(lambda x: x[:400])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ebb745dd1dba91ad6eb45fbffbfe3ed714ec525b"
      },
      "cell_type": "code",
      "source": "df_all.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c881f397440d138316eeff52c9be75c77397fe60"
      },
      "cell_type": "markdown",
      "source": "# Get all unique chars to preform lable encoder"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6264f17c02579ea56a039519931d1815fbf29544"
      },
      "cell_type": "code",
      "source": "prod_all_sentences = df_all['product_title']\nsearch_all_sentences = df_all['search_term']\n\nall_unique_chars = np.unique(np.concatenate((prod_all_sentences.append(search_all_sentences).values), axis=None))\nall_unique_chars = np.append(['<uniq>'], all_unique_chars)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0ea710d85215d52e70835e614b7ced758ff820ea",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "all_unique_chars",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "473df6dae184a3f314280053e42685666fe68731"
      },
      "cell_type": "markdown",
      "source": "# Replace tokens in numbers using lable encoder"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2b96a77ef1cf500ffdffdc1441206d50eb28098"
      },
      "cell_type": "code",
      "source": "def token_to_num(data, unique_valus):\n    le = LabelEncoder()\n    le.fit(unique_valus)\n    data['product_title'] = data['product_title'].apply(lambda char_list: le.transform(char_list).astype(float)) \n    data['search_term'] = data['search_term'].apply(lambda char_list: le.transform(char_list).astype(float)) \n    return data\n\ndf_all = token_to_num(df_all, all_unique_chars)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "73a2a2f6b1a85d932ee2b4971bb8a2522d79330a"
      },
      "cell_type": "code",
      "source": "df_all.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a6b5409fdae553d39ffeb1b427043b1e9ac784a6"
      },
      "cell_type": "markdown",
      "source": "# Check max product description and search term length to know how much padding needed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10927cac267775f265e4e9f780303c1ef35a7db8"
      },
      "cell_type": "code",
      "source": "def get_max_length(data):\n    max_len = 0\n    for i in range(0, len(data)):\n        if len(data.iloc[i]) > max_len:\n            max_len = len(data.iloc[i])\n    return max_len\n\ndef get_avg_length(data):\n    len_sum = 0\n    for i in range(0, len(data)):\n        len_sum += len(data.iloc[i])\n    return int(len_sum / len(data))\n\n# avg_length_prod = get_avg_length(df_all['product_title'])\n# avg_length_search = get_avg_length(df_all['search_term'])\n\nmax_length_prod = get_max_length(df_all['product_title'])\nmax_length_search = get_max_length(df_all['search_term'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5542bea070e62a31ab78c7bf1824e612200c7412"
      },
      "cell_type": "code",
      "source": "# print('avg_length_prod', avg_length_prod)\n# print('avg_length_search', avg_length_search)\n\nprint('max_length_prod', max_length_prod)\nprint('max_length_search', max_length_search)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f82542e5b5cf610e021ef916050be642ba6e9ec6"
      },
      "cell_type": "markdown",
      "source": "# Split back to train and test sets"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "83270d27a0452f0b8687c6ab8c8bf96c95cbc41a"
      },
      "cell_type": "code",
      "source": "df_train = df_all.iloc[:train_size]\ndf_test = df_all.iloc[train_size:]\ndf_test.reset_index(inplace=True, drop=True)\n\ny_data = df_train['relevance']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95068e6c084218fce6316fbbcf73777225c5341f"
      },
      "cell_type": "markdown",
      "source": "# transform relevance to labels"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e065841066e1df4333c542118413897ba2ea5e1e"
      },
      "cell_type": "code",
      "source": "le_labels = LabelEncoder()\nle_labels.fit(y_data.unique())\ny_data_labels = le_labels.transform(y_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aec1a317979d975f0dfb4070b001c08a07fa5f96"
      },
      "cell_type": "markdown",
      "source": "# Split to two sides:\n1. search_term\n2. product_desctiption + title"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6de4691d3ddcd891392b27d16a7d506b34eb9cbb"
      },
      "cell_type": "code",
      "source": "X_data = {'left': df_train['product_title'], 'right': df_train['search_term']}\nX_test = {'left': df_test['product_title'], 'right': df_test['search_term']}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "99022b59ca422f8d72a94d4bd61c86278abccfaa"
      },
      "cell_type": "markdown",
      "source": "# Add zero padding to each char list in size of max_length"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91a87b5b04f00d0019397d4e6df12c9bf0a9a50d"
      },
      "cell_type": "code",
      "source": "for dataset in [X_data, X_test]:\n    dataset['left'] = pad_sequences(dataset['left'], maxlen=max_length_prod)\n    dataset['right'] = pad_sequences(dataset['right'], maxlen=max_length_search)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b80e39c2601bb1a659bdb1909ca33b2317151b4f"
      },
      "cell_type": "code",
      "source": "def reshpe_data(data):\n    data['left'] = np.reshape(data['left'], (data['left'].shape[0], data['left'].shape[1], 1))\n    data['right'] = np.reshape(data['right'], (data['right'].shape[0], data['right'].shape[1], 1))  \n    return data\n\nX_data = reshpe_data(X_data)\nX_test = reshpe_data(X_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7af141c36a3a3e08a846481caf37a9b2c88052dd"
      },
      "cell_type": "code",
      "source": "print('X_train left shape', X_data['left'].shape)\nprint('X_train right shape', X_data['right'].shape)\nprint('y_train shape', y_data.shape)\n\nprint('X_test left shape', X_test['left'].shape)\nprint('X_test right shape', X_test['right'].shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "713de7c64552f64929f44ecb7b018183bf5e44f4"
      },
      "cell_type": "code",
      "source": "def create_onehot(data):\n    onehot_all = []\n    for line in data:\n        onehot_encoded = list()\n        for char in line:\n            onehot = [0 for _ in range(len(all_unique_chars))]\n            onehot[char[0]] = 1\n            onehot_encoded.append(onehot)\n        onehot_all.append(onehot_encoded)\n    return onehot_all\n\nrt = create_onehot(X_data['left'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3b19539db82512790beb9e05de1368af6ba163d"
      },
      "cell_type": "code",
      "source": "for line in X_data['left']:\n    for char in line:\n        print(char)\n        print(char[0])\n        break\n    break",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e88a1554afea52cd69c63ff69c975d42037271ff"
      },
      "cell_type": "markdown",
      "source": "# Build the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2f34839c9c3706a8677726481094437e2f19c8"
      },
      "cell_type": "code",
      "source": "def get_model():\n\n    # input layer\n    left_input = Input(shape=(max_length_prod, 1))\n    right_input = Input(shape=(max_length_search, 1))\n\n    # LSTM layer\n    shared_lstm = LSTM(n_lstm_hidden)\n    left_output = shared_lstm(left_input)\n    right_output = shared_lstm(right_input)\n\n    # concat two outputs\n    concat = concatenate([left_output, right_output])\n    \n    dense_1 = Dense(50, activation=\"relu\")(concat)\n    #dropout_1 = Dropout(0.1)(dense_1)\n\n    # add Dense layer to calculate the similarty between product title and search term\n    output = Dense(1, activation=\"relu\")(dense_1)\n\n    # Pack it all up into a model\n    siamese_model = Model([left_input, right_input], output)\n\n    # colmpiling\n    siamese_model.compile(loss='mse', optimizer='adam')\n    return siamese_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d61c9b10b54462b459d573b29f089aecb2a84530"
      },
      "cell_type": "code",
      "source": "def split_to_train_val(data_x, data_y, train_index, val_index):\n    train_left = data_x['left'][train_index]\n    train_right = data_x['right'][train_index]\n    y_train = data_y[train_index]  \n    val_left = data_x['left'][val_index]\n    val_right = data_x['right'][val_index]\n    y_val = data_y[val_index]\n    \n    return train_left, train_right, val_left, val_right, y_train, y_val",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8003751bc2abcf73d9ea6461755be733f1f5fd23"
      },
      "cell_type": "code",
      "source": "# Plot loss\ndef plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b83054d895105a433c0563fb59ed8d2a4306b86f"
      },
      "cell_type": "code",
      "source": "# Model variables\nn_lstm_hidden = 25\nbatch_size = 256\nn_epoch = 5\n\npred_test = np.zeros(X_test['left'].shape[0])\nmean_rmse = 0\n\nn_splits = 5\n# kfold = KFold(n_splits=n_splits, shuffle=True, random_state=24)\nkfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=24)\n\nfor train_index, test_index in kfold.split(X_data['left'], y_data_labels):    \n    X_train_left_kfold, X_train_right_kfold, X_val_left_kfold, X_val_right_kfold, y_train_kfold, y_val_kfold = split_to_train_val(X_data, y_data.values, train_index, test_index)\n    \n    siamese_model = get_model()\n    history = siamese_model.fit([X_train_left_kfold, X_train_right_kfold], y_train_kfold,\n                            batch_size=batch_size, epochs=n_epoch,\n                            validation_data=([X_val_left_kfold, X_val_right_kfold], y_val_kfold))\n    \n    pred_val = siamese_model.predict([X_val_left_kfold, X_val_right_kfold])\n    rmse = sqrt(mean_squared_error(y_val_kfold, pred_val))\n    mean_rmse += (rmse / n_splits)\n    \n    pred_test += np.reshape((siamese_model.predict([X_test['left'], X_test['right']]) / n_splits), (len(pred_test),))    \n    \n    print('rmse', rmse)\n    plot_loss(history)\n    \nprint('mean rmse', mean_rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ec48d09772a06ec47042d24ce9cea5dfa63ecee"
      },
      "cell_type": "markdown",
      "source": "# Use the model we got as feature extractor \n1. for XGBoost model\n2. for lightgbm model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a464f5fb8aa73d1e27af90e336448cb927eae0d2"
      },
      "cell_type": "code",
      "source": "# Train the model on all the data\nsiamese_model = get_model()\nhistory = siamese_model.fit([X_data['left'], X_data['right']], y_data.values, batch_size=batch_size, epochs=n_epoch)\n\n# get the output of the concat layer and use it as features to the ml models\nconcat_layer = siamese_model.layers[3].output\nfeature_model = Model(siamese_model.input, concat_layer)\nfeature_model.compile(loss='mse', optimizer='adam')\nprint(feature_model.summary())\n\n# we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\nfeaturs = feature_model.predict([X_data['left'], X_data['right']])\n\n# we preform the prediction also on the test set to evaluate the mse on test set\nfeatures_test = feature_model.predict([X_test['left'], X_test['right']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d1512a3193ea30a0c355380709e1df3c13fa8a"
      },
      "cell_type": "code",
      "source": "# # get the output of the concat layer and use it as features to the ml models\n# concat_layer = siamese_model.layers[3].output\n# feature_model = Model(siamese_model.input, concat_layer)\n# feature_model.compile(loss='mse', optimizer='adam')\n# print(feature_model.summary())\n\n# # we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\n# featurs = feature_model.predict([X_train_left_kfold, X_train_right_kfold])\n\n# # we preform the prediction also on the test set to evaluate the mse on test set\n# features_test = feature_model.predict([X_val_left_kfold, X_val_right_kfold])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b0545941c07162e1d4ce35324ec2a1c7c8c1f43"
      },
      "cell_type": "code",
      "source": "# xgb\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.01, gamma=0, subsample=0.8, colsample_bytree=1, max_depth=7)\nxgb_model.fit(featurs, y_data)\nxgb_pred = xgb_model.predict(features_test)\n\n# lgb\nlgb_model = lgb.sklearn.LGBMRegressor(is_unbalance=True, learning_rate =0.01, subsample=0.8, colsample_bytree=0.6, max_depth=7)\nlgb_model.fit(featurs, y_data)\nlgb_pred = lgb_model.predict(features_test)\n\n\n# xgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, xgb_pred))\n# lgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, lgb_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a863db3a06ab2c990ecbc8e4c68816d30a20a915"
      },
      "cell_type": "code",
      "source": "def round_pred(pred):\n    for i in range(0,len(pred)):\n        if pred[i] < 1:\n            pred[i] = 1\n        if pred[i] > 3:\n            pred[i] = 3\n    return pred\n\nxgb_pred = round_pred(xgb_pred.astype(float))\nlgb_pred = round_pred(lgb_predastype(float))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3198b356da0e4e4576616abcc58b245e4d4f04df"
      },
      "cell_type": "code",
      "source": "# print(xgb_rmse, lgb_rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4dffbbe091404ca00cda0de1986165d7742dbf69"
      },
      "cell_type": "markdown",
      "source": "# Write predictions to sample submission file"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0359113ee85810ef14a496c562709e5d35a1d290"
      },
      "cell_type": "code",
      "source": "sample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred_test\nsample_sub.to_csv('sample_submmision_char_siamese.csv', index=False)\n\nsample_sub['relevance'] = xgb_pred\nsample_sub.to_csv('sample_submmision_char_xgb.csv', index=False)\n\nsample_sub['relevance'] = lgb_pred\nsample_sub.to_csv('sample_submmision_char_lgb.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}