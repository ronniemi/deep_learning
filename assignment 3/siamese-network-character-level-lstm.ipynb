{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom time import time\n\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Input, Embedding, Lambda, Dense, concatenate, Flatten, Dropout\nfrom keras.layers.normalization import BatchNormalization\nimport keras.backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport datetime\nfrom time import time\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer, text_to_word_sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nfrom nltk.corpus import stopwords\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"510b1e325351c39896012f8eaae8e870d3cc991a"},"cell_type":"markdown","source":"# Read the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/home-depot-product-search-relevance/train.csv', encoding=\"ISO-8859-1\")\ntest = pd.read_csv('../input/home-depot-product-search-relevance/test.csv', encoding=\"ISO-8859-1\")\nprod_att = pd.read_csv('../input/home-depot-product-search-relevance/attributes.csv', encoding=\"ISO-8859-1\")\ny_test = pd.read_csv('../input/y-test/solution.csv', encoding=\"ISO-8859-1\")\n\nprint('train size', train.shape)\nprint('test size', test.shape)\nprint('prod_att size', prod_att.shape)\nprint('y_test size', y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad0bd936ca36b7bca74de3177f4fe6cf977317e4"},"cell_type":"code","source":"# evently we add this only to calculate MAE on test set\n\ntest = pd.merge(test, y_test, on='id', how='left')\ntest = test.loc[test['Usage'] != 'Ignored']\ntest.drop(['Usage'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"920a26c0a9ce905505be5ae5a86de6233c8a863e"},"cell_type":"code","source":"train_size = train.shape[0]\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2b295c37257082cf282bbe3a85bff98a996aa42d"},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"620dccbc2a87ffc35fc7494a1a6ab00c9ab1166d"},"cell_type":"code","source":"prod_att_metrial = prod_att.loc[prod_att['name'] == 'Material'][['product_uid', 'value']].drop_duplicates(['product_uid'])\nprod_att_brand = prod_att.loc[prod_att['name'] == 'MFG Brand Name'][['product_uid', 'value']].drop_duplicates(['product_uid'])\n\ndef merge_att(att_df):\n    global df_all\n    df_all = pd.merge(df_all, att_df, on='product_uid', how='left')\n    df_all = df_all.fillna('')\n    df_all['product_title'] = df_all['product_title'] + ' ' + df_all['value']\n    df_all.drop(['value'], axis=1, inplace=True)\n    return df_all\n\ndf_all = merge_att(prod_att_metrial)\ndf_all = merge_att(prod_att_brand)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"389705623e39b429848f8c9c3c0764a4fb7d84d9"},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12a8f2bc529733c5db6c296b323c670a66ab2c65"},"cell_type":"code","source":"# tokenizer = Tokenizer(num_words=50000, filters='!\"$&()*+,-.:;<=>?[\\]^_`{|}~')\n# tokenizer.fit_on_texts(list(df_all['product_title'].append(df_all['search_term']).values))\n# vocab = tokenizer.word_counts\n# mean_freq = np.mean(np.array(list(vocab.values())))\n\n# for i in range (0, len(df_all['product_title'])):\n#     words = text_to_word_sequence(df_all['product_title'][i], filters='!\"$&()*+,-.:;<=>?[\\]^_`{|}~')\n#     for j in range(0, len(words)):\n#         if vocab[words[j]] < mean_freq * 1.5:\n#             words[j] = ''\n#     df_all['product_title'][i] = ' '.join(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":true,"_uuid":"d998e3d1e328cbf1f333420d9c5c2b9b8921ab1c"},"cell_type":"code","source":"# stop_words = stopwords.words('english')\n\n# df_all['product_title'] = df_all['product_title'].apply(lambda sent: ' '.join([word for word in text_to_word_sequence(sent, filters='!\"$&()*+,-.:;<=>?[\\]^_`{|}~') \n#                                                                       if word not in stopwords.words('english')]))\n# # df_all['search_term'] = df_all['search_term'].apply(lambda sent: [lower_char(character) for character in sent if character not in char_to_remove])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7b49aaa94c7188202018356fafe950c882945da3"},"cell_type":"markdown","source":"# Split product_titile and search_term to chars"},{"metadata":{"trusted":true,"_uuid":"0ddb5429f76f21507438b4f34f3ff7166cae5621"},"cell_type":"code","source":"char_to_remove = [' ', '{', '}', '\"', '(', ')', '.', ',', '&','[',']','`','_','\\'', '~', '\\\\', '-', ':', ';', '=', '?',\n                  '\\x80',  '\\x81', '\\x82', '\\x84', '\\x89', '\\x8b', '\\x90', '\\x93', '\\x95', '\\x99', '\\x9a', '!',\n                  '\\x9d', '\\xa0', '¡', '¢', '®', 'ª', 'À', 'Ã', 'Â', 'È', 'Ê', 'Ë', 'Ï', 'Ò', 'Û', \n                  'Ü', 'â', 'ã', 'å', 'è', '÷', '$', '*']\n\ndef lower_char(char):\n    if char >= 'A' and char <= 'Z':\n        return char.lower()\n    else:\n        return char\n\ndf_all['product_title'] = df_all['product_title'].apply(lambda sent: [lower_char(character) for character in sent if character not in char_to_remove])\ndf_all['search_term'] = df_all['search_term'].apply(lambda sent: [lower_char(character) for character in sent if character not in char_to_remove])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ebb745dd1dba91ad6eb45fbffbfe3ed714ec525b"},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c881f397440d138316eeff52c9be75c77397fe60"},"cell_type":"markdown","source":"# Get all unique chars to preform lable encoder"},{"metadata":{"trusted":true,"_uuid":"6264f17c02579ea56a039519931d1815fbf29544"},"cell_type":"code","source":"prod_all_sentences = df_all['product_title']\nsearch_all_sentences = df_all['search_term']\n\nall_unique_chars = np.unique(np.concatenate((prod_all_sentences.append(search_all_sentences).values), axis=None))\nall_unique_chars = np.append(['<uniq>'], all_unique_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ea710d85215d52e70835e614b7ced758ff820ea","scrolled":true},"cell_type":"code","source":"all_unique_chars","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"473df6dae184a3f314280053e42685666fe68731"},"cell_type":"markdown","source":"# Replace tokens in numbers using lable encoder"},{"metadata":{"trusted":true,"_uuid":"d2b96a77ef1cf500ffdffdc1441206d50eb28098"},"cell_type":"code","source":"def token_to_num(data, unique_valus):\n    le = LabelEncoder()\n    le.fit(unique_valus)\n    data['product_title'] = data['product_title'].apply(lambda char_list: le.transform(char_list).astype(float)) \n    data['search_term'] = data['search_term'].apply(lambda char_list: le.transform(char_list).astype(float)) \n    return data\n\ndf_all = token_to_num(df_all, all_unique_chars)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a2a2f6b1a85d932ee2b4971bb8a2522d79330a"},"cell_type":"code","source":"df_all.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a6b5409fdae553d39ffeb1b427043b1e9ac784a6"},"cell_type":"markdown","source":"# Check max product description and search term length to know how much padding needed"},{"metadata":{"trusted":true,"_uuid":"10927cac267775f265e4e9f780303c1ef35a7db8"},"cell_type":"code","source":"def get_max_length(data):\n    max_len = 0\n    for i in range(0, len(data)):\n        if len(data.iloc[i]) > max_len:\n            max_len = len(data.iloc[i])\n    return max_len\n\ndef get_avg_length(data):\n    len_sum = 0\n    for i in range(0, len(data)):\n        len_sum += len(data.iloc[i])\n    return int(len_sum / len(data))\n\navg_length_prod = get_avg_length(df_all['product_title'])\navg_length_search = get_avg_length(df_all['search_term'])\n\nmax_length_prod = get_max_length(df_all['product_title'])\nmax_length_search = get_max_length(df_all['search_term'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5542bea070e62a31ab78c7bf1824e612200c7412"},"cell_type":"code","source":"print('avg_length_prod', avg_length_prod)\nprint('avg_length_search', avg_length_search)\n\nprint('max_length_prod', max_length_prod)\nprint('max_length_search', max_length_search)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f82542e5b5cf610e021ef916050be642ba6e9ec6"},"cell_type":"markdown","source":"# Split back to train and test sets"},{"metadata":{"trusted":true,"_uuid":"83270d27a0452f0b8687c6ab8c8bf96c95cbc41a"},"cell_type":"code","source":"df_train = df_all.iloc[:train_size]\ndf_test = df_all.iloc[train_size:]\ndf_test.reset_index(inplace=True, drop=True)\n\ny_data = df_train['relevance']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"95068e6c084218fce6316fbbcf73777225c5341f"},"cell_type":"markdown","source":"# transform relevance to labels"},{"metadata":{"trusted":true,"_uuid":"e065841066e1df4333c542118413897ba2ea5e1e"},"cell_type":"code","source":"le_labels = LabelEncoder()\nle_labels.fit(y_data.unique())\ny_data_labels = le_labels.transform(y_data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"aec1a317979d975f0dfb4070b001c08a07fa5f96"},"cell_type":"markdown","source":"# Split to two sides:\n1. search_term\n2. product_desctiption + title"},{"metadata":{"trusted":true,"_uuid":"6de4691d3ddcd891392b27d16a7d506b34eb9cbb"},"cell_type":"code","source":"X_data = {'left': df_train['product_title'], 'right': df_train['search_term']}\nX_test = {'left': df_test['product_title'], 'right': df_test['search_term']}","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"99022b59ca422f8d72a94d4bd61c86278abccfaa"},"cell_type":"markdown","source":"# Add zero padding to each char list in size of max_length"},{"metadata":{"trusted":true,"_uuid":"91a87b5b04f00d0019397d4e6df12c9bf0a9a50d"},"cell_type":"code","source":"for dataset in [X_data, X_test]:\n    dataset['left'] = pad_sequences(dataset['left'], maxlen=avg_length_prod)\n    dataset['right'] = pad_sequences(dataset['right'], maxlen=avg_length_search)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"55b6bcff5b0b3e38daf78b577eca8805890a533a"},"cell_type":"code","source":"# def reshpe_data(data):\n#     data['left'] = np.reshape(data['left'], (data['left'].shape[0], data['left'].shape[1], 1))\n#     data['right'] = np.reshape(data['right'], (data['right'].shape[0], data['right'].shape[1], 1))  \n#     return data\n\n# X_data = reshpe_data(X_data)\n# X_test = reshpe_data(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1640c0702fe0f8e667c1ff07ba88e084ad4ab247"},"cell_type":"markdown","source":"# Normalized the label"},{"metadata":{"trusted":true,"_uuid":"dd71fa292c2babc1c9fb6d05dff15698a370667f"},"cell_type":"code","source":"# mean_val = y_data.mean()\n# std_val = y_data.std()\n# y_data_norm = (y_data - mean_val) / std_val\n\nmin_val = y_data.min()\nmax_val = y_data.max()\ny_data_norm = (y_data - min_val) / (max_val - min_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e60c327bd8667b712cb91df9e833b0af24f33ba7"},"cell_type":"markdown","source":"# Create one-hot to each char"},{"metadata":{"trusted":true,"_uuid":"163ae0dafbe6603c4b75f2f8e0af5d428f1e3757"},"cell_type":"code","source":"def create_onehot(data):\n    onehot_all = []\n    for line in data:\n        onehot_encoded = []\n        for char in line:\n            onehot = np.zeros(len(all_unique_chars)) #[0 for _ in range(len(all_unique_chars))]\n            onehot[char] = 1\n            onehot_encoded.append(onehot)\n        onehot_all.append(onehot_encoded)\n    return np.reshape(onehot_all, (len(onehot_all), len(onehot_all[0]), len(onehot_all[0][0])))\n\nX_data['left'] = create_onehot(X_data['left'])\nX_data['right'] = create_onehot(X_data['right'])\nX_test['left'] = create_onehot(X_test['left'])\nX_test['right'] = create_onehot(X_test['right'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ec74114857a9b6891f657fb1ac7fc7e5bda01cfa"},"cell_type":"code","source":"print('X_train left shape', X_data['left'].shape)\nprint('X_train right shape', X_data['right'].shape)\nprint('y_train shape', y_data.shape)\n\nprint('X_test left shape', X_test['left'].shape)\nprint('X_test right shape', X_test['right'].shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"256587286446a9d42994eb5f6ae07bbfbc876155"},"cell_type":"markdown","source":"# Cleare space in memory"},{"metadata":{"trusted":true,"_uuid":"8cb937f14761ee9c3ccbce20c3c145fc3ee34808"},"cell_type":"code","source":"del train\ndel prod_att\ndel df_all\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e88a1554afea52cd69c63ff69c975d42037271ff"},"cell_type":"markdown","source":"# Build the model"},{"metadata":{"trusted":true,"_uuid":"0a2f34839c9c3706a8677726481094437e2f19c8"},"cell_type":"code","source":"# Model variables\nn_lstm_hidden = 25\nbatch_size = 128\nn_epoch = 5\n\ndef get_model():\n\n    # input layer\n    left_input = Input(shape=(avg_length_prod, len(all_unique_chars)))\n    right_input = Input(shape=(avg_length_search, len(all_unique_chars)))\n\n    # LSTM layer\n    shared_lstm = LSTM(n_lstm_hidden)\n    left_output = shared_lstm(left_input)\n    right_output = shared_lstm(right_input)\n\n    # batch normalization layer\n    left_output = BatchNormalization()(left_output)\n    right_output = BatchNormalization()(right_output)\n    \n    # concat two outputs\n    concat = concatenate([left_output, right_output])\n    \n    dense_1 = Dense(50, activation=\"relu\")(concat)\n    dropout_1 = Dropout(0.1)(dense_1)\n\n    # add Dense layer to calculate the similarty between product title and search term\n    output = Dense(1, activation=\"relu\")(dropout_1)\n\n    # Pack it all up into a model\n    siamese_model = Model([left_input, right_input], output)\n\n    # colmpiling\n    siamese_model.compile(loss='mse', optimizer='adam')\n    return siamese_model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d61c9b10b54462b459d573b29f089aecb2a84530"},"cell_type":"code","source":"def split_to_train_val(data_x, data_y, train_index, val_index):\n    train_left = data_x['left'][train_index]\n    train_right = data_x['right'][train_index]\n    y_train = data_y[train_index]  \n    val_left = data_x['left'][val_index]\n    val_right = data_x['right'][val_index]\n    y_val = data_y[val_index]\n    \n    return train_left, train_right, val_left, val_right, y_train, y_val","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8003751bc2abcf73d9ea6461755be733f1f5fd23"},"cell_type":"code","source":"# Plot loss\ndef plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ac000601771b6e7a3f32e1ec6a17650e9191af99"},"cell_type":"code","source":"def plot_fiting(y_pred, y_true):\n    n_samples = 100\n    axis=[x for x in range(n_samples)]\n    plt.plot(axis, y_true[:n_samples], marker='.', label=\"actual\")\n    plt.plot(axis, y_pred[:n_samples], 'r', label=\"prediction\")\n    plt.ylabel('Relevance', size=15)\n    plt.xlabel('Time step', size=15)\n    plt.legend(fontsize=15)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a62057d15f01b2ee8967c8a7cac224e2ce7911a"},"cell_type":"code","source":"mean_mae_train = 0\nmean_rmse_train = 0\nmean_mae_val = 0\nmean_rmse_val = 0\n\ndef eval_mean(y_true, y_pred, is_val=True):\n    global mean_mae_train, mean_rmse_train, mean_mae_val, mean_rmse_val\n    \n    y_true_norm = (y_true * (max_val - min_val)) + min_val\n    y_pred_norm = (y_pred * (max_val - min_val)) + min_val\n    \n    mae_value = mean_absolute_error(y_true_norm, y_pred_norm)\n    rmse_value = sqrt(mean_squared_error(y_true_norm, y_pred_norm))\n    \n    if is_val:\n        mean_mae_val += (mae_value / n_splits)\n        mean_rmse_val += (rmse_value / n_splits)\n    else:\n        mean_mae_train += (mae_value / n_splits)\n        mean_rmse_train += (rmse_value / n_splits)\n        \n    print('mae_value', mae_value)\n    print('rmse_value', rmse_value)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4baa9b23841de454c8563cebc06adf3b069d9dc4"},"cell_type":"markdown","source":"# StratifiedKFold cross validation"},{"metadata":{"trusted":true,"_uuid":"b83054d895105a433c0563fb59ed8d2a4306b86f"},"cell_type":"code","source":"pred_test = np.zeros(X_test['left'].shape[0])\n\nn_splits = 2\n# kfold = KFold(n_splits=n_splits, shuffle=True, random_state=24)\nkfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=24)\n\nmean_train_time = 0\nmean_pred_time = 0\nstart_time = time()\n\nfor train_index, test_index in kfold.split(X_data['left'], y_data_labels):    \n    X_train_left_kfold, X_train_right_kfold, X_val_left_kfold, X_val_right_kfold, y_train_kfold, y_val_kfold = split_to_train_val(X_data, y_data_norm.values, train_index, test_index)\n    \n    start = time()\n    siamese_model = get_model()\n    history = siamese_model.fit([X_train_left_kfold, X_train_right_kfold], y_train_kfold,\n                            batch_size=batch_size, epochs=n_epoch,\n                            validation_data=([X_val_left_kfold, X_val_right_kfold], y_val_kfold))\n    mean_train_time += (round(time()-start, 3) / n_splits)\n    \n    pred_train = siamese_model.predict([X_train_left_kfold, X_train_right_kfold])\n    pred_val = siamese_model.predict([X_val_left_kfold, X_val_right_kfold])\n    eval_mean(y_train_kfold, pred_train, False)\n    eval_mean(y_val_kfold, pred_val, True)\n    \n    start = time()\n    pred_test += np.reshape((siamese_model.predict([X_test['left'], X_test['right']]) / n_splits), (len(pred_test),))   \n    mean_pred_time += (round(time()-start, 3) / n_splits)\n    \n    plot_loss(history)\n    plot_fiting(pred_train, y_train_kfold)\n    plot_fiting(pred_val, y_val_kfold)\n    \nprint('mean_mae_train', mean_mae_train)\nprint('mean_rmse_train', mean_rmse_train)\nprint('mean_mae_val', mean_mae_val)\nprint('mean_rmse_val', mean_rmse_val)\nprint('end time', round(time()-start_time, 3))\nprint('mean train time', mean_train_time)\nprint('mean pred time', mean_pred_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4ec48d09772a06ec47042d24ce9cea5dfa63ecee"},"cell_type":"markdown","source":"# Use the model we got as feature extractor \n1. for XGBoost model\n2. for lightgbm model"},{"metadata":{"trusted":true,"_uuid":"7fb90ac201200c27df7bc74da8403c95537c147e"},"cell_type":"code","source":"# split to train and validation sets\ntrain_indx = int(0.8*X_data['left'].shape[0])\nX_train_left = X_data['left'][:train_indx]\nX_val_left = X_data['left'][train_indx:]\nX_train_right = X_data['right'][:train_indx]\nX_val_right = X_data['right'][train_indx:]\ny_train = y_data_norm[:train_indx]\ny_val = y_data_norm[train_indx:]\n\nprint('X_train_left shape', X_train_left.shape)\nprint('X_val_left shape', X_val_left.shape)\nprint('X_train_right shape', X_train_right.shape)\nprint('X_val_right shape', X_val_right.shape)\nprint('y_train shape', y_train.shape)\nprint('y_val shape', y_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3b464fba8d43dfeffc1de0bf3433a9bf44a947a5"},"cell_type":"code","source":"# Train the model on train and then predict train, val and test to get features\nsiamese_model = get_model()\nhistory = siamese_model.fit([X_train_left, X_train_right], y_train.values, batch_size=batch_size, epochs=n_epoch)\n\n# get the output of the concat layer and use it as features to the ml models\nconcat_layer = siamese_model.layers[5].output\nfeature_model = Model(siamese_model.input, concat_layer)\nfeature_model.compile(loss='mse', optimizer='adam')\nprint(feature_model.summary())\n\n# we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\nfeaturs = feature_model.predict([X_train_left, X_train_right])\n\n# we preform the prediction also on the test set to evaluate rmse and mae on val set\nfeatures_val = feature_model.predict([X_val_left, X_val_right])\n\n# we preform the prediction also on the test set to evaluate the mse on test set\nfeatures_test = feature_model.predict([X_test['left'], X_test['right']])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b0545941c07162e1d4ce35324ec2a1c7c8c1f43"},"cell_type":"code","source":"# xgb\nstart1 = time()\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.01, gamma=0, subsample=0.6, colsample_bytree=1, max_depth=3)\nxgb_model.fit(featurs, y_train)\nprint(\"xgb train time\", round(time()-start1, 3))\n\nstart2 = time()\nxgb_pred_test = xgb_model.predict(features_test)\nprint(\"xgb predict time\", round(time()-start2, 3))\nprint(\"xgb total time\", round(time()-start1, 3))\nxgb_pred_val = xgb_model.predict(features_val)\nxgb_pred_train = xgb_model.predict(featurs)\n\n# lgb\nstart1 = time()\nlgb_model = lgb.sklearn.LGBMRegressor(is_unbalance=True, learning_rate =0.01, subsample=0.6, colsample_bytree=0.6, max_depth=5, num_leaves=60)\nlgb_model.fit(featurs, y_train)\nprint(\"lgb train time\", round(time()-start1, 3))\n\nstart2 = time()\nlgb_pred_test = lgb_model.predict(features_test)\nprint(\"lgb predict time\", round(time()-start2, 3))\nprint(\"lgb total time\", round(time()-start1, 3))\nlgb_pred_val = lgb_model.predict(features_val)\nlgb_pred_train = lgb_model.predict(featurs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a863db3a06ab2c990ecbc8e4c68816d30a20a915"},"cell_type":"code","source":"def round_pred(pred):\n    pred = (pred * (max_val - min_val)) + min_val\n    for i in range(0,len(pred)):\n        if pred[i] < 1:\n            pred[i] = 1\n        if pred[i] > 3:\n            pred[i] = 3\n    return pred\n\ndef eval_festure(y_true, y_pred, title):\n    print(title, 'mae', mean_absolute_error(y_true, y_pred))\n    print(title, 'rmse', sqrt(mean_squared_error(y_true, y_pred)))\n\nxgb_pred_test = round_pred(xgb_pred_test.astype(float))\nlgb_pred_test = round_pred(lgb_pred_test.astype(float))\n\nxgb_pred_train = round_pred(xgb_pred_train.astype(float))\nlgb_pred_train = round_pred(lgb_pred_train.astype(float))\n\nxgb_pred_val = round_pred(xgb_pred_val.astype(float))\nlgb_pred_val = round_pred(lgb_pred_val.astype(float))\n\npred_test = (pred_test * (max_val - min_val)) + min_val\n\neval_festure(test['relevance'], pred_test, 'pred_test')\neval_festure(test['relevance'], lgb_pred_test, 'lgb_pred_test')\neval_festure(y_train, lgb_pred_train, 'lgb_pred_train')\neval_festure(y_val, lgb_pred_val, 'lgb_pred_val')\neval_festure(test['relevance'], xgb_pred_test, 'xgb_pred_test')\neval_festure(y_train, xgb_pred_train, 'xgb_pred_train')\neval_festure(y_val, xgb_pred_val, 'xgb_pred_val')\n\nprint('siamese test')\nplot_fiting(test['relevance'], pred_test)\nprint('lgb test')\nplot_fiting(test['relevance'], lgb_pred_test)\nprint('xgb test')\nplot_fiting(test['relevance'], xgb_pred_test)\n\nprint('lgb train')\nplot_fiting(y_train, lgb_pred_train)\nprint('xgb train')\nplot_fiting(y_train, xgb_pred_train)\n\nprint('lgb val')\nplot_fiting(y_val, lgb_pred_val)\nprint('xgb val')\nplot_fiting(y_val, xgb_pred_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4dffbbe091404ca00cda0de1986165d7742dbf69"},"cell_type":"markdown","source":"# Write predictions to sample submission file"},{"metadata":{"trusted":true,"_uuid":"0359113ee85810ef14a496c562709e5d35a1d290"},"cell_type":"code","source":"sample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred_test\nsample_sub.to_csv('sample_submmision_char_siamese.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5263aaf980efc004ccbeb9fb6adfc851b85871a9"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}