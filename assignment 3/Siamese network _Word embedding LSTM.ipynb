{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Input, Embedding, Lambda, Dense, concatenate, Flatten, Dropout\nfrom keras.layers.normalization import BatchNormalization\nimport keras.backend as K\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import StratifiedKFold, KFold,cross_val_score\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport datetime\nfrom time import time\nimport gc\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "510b1e325351c39896012f8eaae8e870d3cc991a"
      },
      "cell_type": "markdown",
      "source": "# Read the data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train = pd.read_csv('../input/home-depot-product-search-relevance/train.csv', encoding=\"ISO-8859-1\")\ntest = pd.read_csv('../input/home-depot-product-search-relevance/test.csv', encoding=\"ISO-8859-1\")\n# prod = pd.read_csv('../input/home-depot-product-search-relevance/product_descriptions.csv')\n\nprint('train size', train.shape)\nprint('test size', test.shape)\n# print('prod size', prod.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "920a26c0a9ce905505be5ae5a86de6233c8a863e"
      },
      "cell_type": "code",
      "source": "train_size = train.shape[0]\ndf_all = pd.concat((train, test), axis=0, ignore_index=True)\n# df_all = pd.merge(df_all, prod, on='product_uid', how='left')\n# df_all['product_title'] = (df_all['product_title'] + df_all['product_description'])\n# df_all.drop(['product_description', 'product_uid', 'id'], axis=1, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b295c37257082cf282bbe3a85bff98a996aa42d"
      },
      "cell_type": "code",
      "source": "df_all.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "72c5c3e31a1625ed2c6dd2f2f5352ffb396f1984"
      },
      "cell_type": "markdown",
      "source": "# Difine labels"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e9c0fcdd3c80b97fadb3eef4c6d54e5a57c3e4a9"
      },
      "cell_type": "code",
      "source": "y_data = train['relevance']\n\n# transform relevance to labels\nle_labels = LabelEncoder()\nle_labels.fit(y_data.unique())\ny_data_labels = le_labels.transform(y_data)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "648d014248f575ff76a50465f31c92b6edbeb56a"
      },
      "cell_type": "markdown",
      "source": "# Check max product description and search term length to know how much padding needed"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b399f090841e148561c7f14256e52fa4f946b1a1"
      },
      "cell_type": "code",
      "source": "def get_max_length(data):\n    max_len = 0\n    for i in range(0, len(data)):\n        n_words = len(data.iloc[i].split())\n        if n_words > max_len:\n            max_len = n_words\n    return max_len\n\nmax_length_prod = get_max_length(df_all['product_title'])\nmax_length_search = get_max_length(df_all['search_term'])\n\nprint('max_length_prod', max_length_prod)\nprint('max_length_search', max_length_search)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6a1dad7c20f6e167fa6d00f9d4290e1e2ce383b"
      },
      "cell_type": "markdown",
      "source": "# Word embedding"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "56096b03c2aa3d396b845802e90415da5cb5e589"
      },
      "cell_type": "code",
      "source": "embed_size = 300\nmax_features = 50000 \n\n# Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features, filters='!\"$&()*+,-.:;<=>?[\\]^_`{|}~')\ntokenizer.fit_on_texts(list(df_all['product_title'].append(df_all['search_term']).values))\n\nX_train_prod = tokenizer.texts_to_sequences(train['product_title'].values)\nX_train_search = tokenizer.texts_to_sequences(train['search_term'].values)\nX_test_prod = tokenizer.texts_to_sequences(test['product_title'].values)\nX_test_search = tokenizer.texts_to_sequences(test['search_term'].values)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5b691cb1a543d0ad4cb2ef8e03c853f997071456"
      },
      "cell_type": "markdown",
      "source": "# Padding by the max_length_prod and max_length_search"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da8304d8b1ef5bbcb6dfcdba45d3a5f3286501d4"
      },
      "cell_type": "code",
      "source": "max_length = max(max_length_prod, max_length_search)\n\nX_train_prod = pad_sequences(X_train_prod, maxlen=max_length)\nX_train_search = pad_sequences(X_train_search, maxlen=max_length)\nX_test_prod = pad_sequences(X_test_prod, maxlen=max_length)\nX_test_search = pad_sequences(X_test_search, maxlen=max_length)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b92ef9663b8889e61b6f36f45c1991cb99bbe13d"
      },
      "cell_type": "markdown",
      "source": "# Assign embedding to words by GoogleNews-vectors-negative300 embedding "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f4739806c196429a563d11056338e225e6e42aa"
      },
      "cell_type": "code",
      "source": "EMBEDDING_FILE = '../input/gnewsvector/GoogleNews-vectors-negative300.bin'\nembeddings_index = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index)) + 1\nembedding_matrix = (np.random.rand(nb_words, embed_size) - 0.5) / 5.0\nembedding_matrix[0] = 0\nfor word, i in word_index.items():\n    if i >= max_features: \n        continue\n    if word in embeddings_index:\n        embedding_matrix[i] = embeddings_index.get_vector(word)\n\ndel EMBEDDING_FILE\ngc.collect()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e88a1554afea52cd69c63ff69c975d42037271ff"
      },
      "cell_type": "markdown",
      "source": "# Build the model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0a2f34839c9c3706a8677726481094437e2f19c8"
      },
      "cell_type": "code",
      "source": "# Model variables\nn_lstm_hidden = 25\nbatch_size = 256\nn_epoch = 5\n\ndef get_model():\n\n    # input layer\n    left_input = Input(shape=(max_length, ))\n    right_input = Input(shape=(max_length, ))\n    \n    # embedding layers\n    embedding_layer = Embedding(nb_words, output_dim=embed_size, input_length=max_length, weights=[embedding_matrix])\n\n    # Embedded version of the inputs\n    encoded_left = embedding_layer(left_input)\n    encoded_right = embedding_layer(right_input)\n\n    # LSTM layer\n    shared_lstm = LSTM(n_lstm_hidden)\n    left_output = shared_lstm(encoded_left)\n    right_output = shared_lstm(encoded_right)\n\n    # concat two outputs\n    concat = concatenate([left_output, right_output])\n    \n    dense_1 = Dense(32, activation=\"relu\")(concat)\n    dropout_1 = Dropout(0.3)(dense_1)\n\n    # add Dense layer to calculate the similarty between product title and search term\n    output = Dense(1, activation=\"relu\")(dropout_1)\n\n    # Pack it all up into a model\n    siamese_model = Model([left_input, right_input], output)\n\n    # colmpiling\n    siamese_model.compile(loss='mse', optimizer='adam')\n    return siamese_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d61c9b10b54462b459d573b29f089aecb2a84530"
      },
      "cell_type": "code",
      "source": "def split_to_train_val(data_x, train_index, val_index):\n    X_train = data_x[train_index]\n    X_val = data_x[val_index]\n    \n    return X_train, X_val",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8003751bc2abcf73d9ea6461755be733f1f5fd23"
      },
      "cell_type": "code",
      "source": "# Plot loss\ndef plot_loss(history):\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper right')\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "271e343fd598ed7d29d994cbf5ccc80b6695298b"
      },
      "cell_type": "code",
      "source": "pred_test = np.zeros(X_test_prod.shape[0])\nmean_rmse = 0\n\nn_splits = 5\n# kfold = KFold(n_splits=n_splits, shuffle=True, random_state=24)\nkfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=24)\n\nfor train_index, val_index in kfold.split(X_train_prod, y_data_labels):\n    y_train_kfold = y_data[train_index] \n    y_val_kfold = y_data[val_index]\n    X_train_left_kfold, X_val_left_kfold = split_to_train_val(X_train_prod, train_index, val_index)\n    X_train_right_kfold, X_val_right_kfold = split_to_train_val(X_train_search, train_index, val_index)\n    \n    siamese_model = get_model()\n    history = siamese_model.fit([X_train_left_kfold, X_train_right_kfold], y_train_kfold,\n                            batch_size=batch_size, epochs=n_epoch,\n                            validation_data=([X_val_left_kfold, X_val_right_kfold], y_val_kfold))\n    \n    pred_val = siamese_model.predict([X_val_left_kfold, X_val_right_kfold])\n    rmse = sqrt(mean_squared_error(y_val_kfold, pred_val))\n    mean_rmse += (rmse / n_splits)\n    \n    pred_test += np.reshape((siamese_model.predict([X_test_prod, X_test_search]) / n_splits), (len(pred_test),))    \n    \n    print('rmse', rmse)\n    plot_loss(history)\n    \nprint('mean rmse', mean_rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ec48d09772a06ec47042d24ce9cea5dfa63ecee"
      },
      "cell_type": "markdown",
      "source": "# Use the model we got as feature extractor \n1. for XGBoost model\n2. for lightgbm model"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a464f5fb8aa73d1e27af90e336448cb927eae0d2"
      },
      "cell_type": "code",
      "source": "# Train the model on all the data\nsiamese_model = get_model()\nhistory = siamese_model.fit([X_data['left'], X_data['right']], y_data.values, batch_size=batch_size, epochs=n_epoch)\n\n# get the output of the concat layer and use it as features to the ml models\nconcat_layer = siamese_model.layers[3].output\nfeature_model = Model(siamese_model.input, concat_layer)\nfeature_model.compile(loss='mse', optimizer='adam')\nprint(feature_model.summary())\n\n# we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\nfeaturs = feature_model.predict([X_data['left'], X_data['right']])\n\n# we preform the prediction also on the test set to evaluate the mse on test set\nfeatures_test = feature_model.predict([X_test['left'], X_test['right']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07d1512a3193ea30a0c355380709e1df3c13fa8a"
      },
      "cell_type": "code",
      "source": "# # get the output of the concat layer and use it as features to the ml models\n# concat_layer = siamese_model.layers[3].output\n# feature_model = Model(siamese_model.input, concat_layer)\n# feature_model.compile(loss='mse', optimizer='adam')\n# print(feature_model.summary())\n\n# # we use the output of the concat layer as fetures so they will be the input to the xgb and lgb models\n# featurs = feature_model.predict([X_train_left_kfold, X_train_right_kfold])\n\n# # we preform the prediction also on the test set to evaluate the mse on test set\n# features_test = feature_model.predict([X_val_left_kfold, X_val_right_kfold])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1b0545941c07162e1d4ce35324ec2a1c7c8c1f43"
      },
      "cell_type": "code",
      "source": "# xgb\nxgb_model = XGBRegressor(n_estimators=100, learning_rate=0.01, gamma=0, subsample=0.8, colsample_bytree=1, max_depth=7)\nxgb_model.fit(featurs, y_data)\nxgb_pred = xgb_model.predict(features_test)\n\n# lgb\nlgb_model = lgb.sklearn.LGBMRegressor(is_unbalance=True, learning_rate =0.01, subsample=0.8, colsample_bytree=0.6, max_depth=7)\nlgb_model.fit(featurs, y_data)\nlgb_pred = lgb_model.predict(features_test)\n\n\n# xgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, xgb_pred))\n# lgb_rmse = np.sqrt(mean_squared_error(y_val_kfold, lgb_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a863db3a06ab2c990ecbc8e4c68816d30a20a915"
      },
      "cell_type": "code",
      "source": "def round_pred(pred):\n    for i in range(0,len(pred)):\n        if pred[i] < 1:\n            pred[i] = 1\n        if pred[i] > 3:\n            pred[i] = 3\n    return pred\n\nxgb_pred = round_pred(xgb_pred.astype(float))\nlgb_pred = round_pred(lgb_predastype(float))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3198b356da0e4e4576616abcc58b245e4d4f04df"
      },
      "cell_type": "code",
      "source": "# print(xgb_rmse, lgb_rmse)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4dffbbe091404ca00cda0de1986165d7742dbf69"
      },
      "cell_type": "markdown",
      "source": "# Write predictions to sample submission file"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0359113ee85810ef14a496c562709e5d35a1d290"
      },
      "cell_type": "code",
      "source": "sample_sub = pd.DataFrame()\nsample_sub['id'] = test['id']\nsample_sub['relevance'] = pred_test\nsample_sub.to_csv('sample_submmision_char_siamese.csv', index=False)\n\nsample_sub['relevance'] = xgb_pred\nsample_sub.to_csv('sample_submmision_char_xgb.csv', index=False)\n\nsample_sub['relevance'] = lgb_pred\nsample_sub.to_csv('sample_submmision_char_lgb.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}